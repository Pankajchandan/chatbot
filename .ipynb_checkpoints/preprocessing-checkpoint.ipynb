{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk.data\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False):\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split() \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #    a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # \n",
    "    # 4. Remove stop words\n",
    "    if remove_stopwords:\n",
    "        meaningful_words = [w for w in words if not w in stops]  \n",
    "    else:\n",
    "        meaningful_words = words\n",
    "    # \n",
    "    # 5. autocorrect spellings\n",
    "    #auto_correct = [spell(w) for w in meaningful_words]\n",
    "    # \n",
    "    # 6. use stemmer to stem\n",
    "    stem_words = [snowball_stemmer.stem(w) for w in meaningful_words]\n",
    "    #\n",
    "    # 7. use lemmatizer to lemmatize the words\n",
    "    lemma_words = [wordnet_lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    #\n",
    "    # 8. Join the words back into one string separated by space, \n",
    "    #    and return the result.\n",
    "    return(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "# we needd sentences because word2vec takes sentences as input. It leverages the SBD\n",
    "def text_to_sentences(text, tokenizer,remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call text_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(raw_sentence,remove_stopwords))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_and_clean_sentences(df):\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "    print (\"Parsing sentences from training set\")\n",
    "    for text in df[\"text\"]:\n",
    "        sentences += text_to_sentences(text, tokenizer)\n",
    "    print (\"parsing done!\")   \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word2vec(df, num_features = 300, min_word_count = 1, num_workers = 4, context = 4, downsampling = 1e-3):\n",
    "    \n",
    "    from gensim.models import word2vec\n",
    "    # parse and clean sentence\n",
    "    sentences = parse_and_clean_sentences(df)\n",
    "    \n",
    "    #initialize logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "\n",
    "    print (\"Training model...\")\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers,size=num_features, min_count = min_word_count,\n",
    "                              window = context, sample = downsampling)\n",
    "\n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # save the model for later use.can load it later using Word2Vec.load()\n",
    "    model_name = \"trainedWord2vecmodel\"\n",
    "    model.save(model_name)\n",
    "    print (\"model saved as\", model_name)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##A simple way to assign a word2vec vector to a document is to take a mean of its words.\n",
    "def makeFeatureVec(words, model, seq_len, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty 3D numpy array (for speed)\n",
    "    featureVec = np.zeros((seq_len, num_features, 1),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            featureVec[nwords] = np.array(model[word]).reshape(num_features,1)\n",
    "            nwords = nwords + 1\n",
    "            \n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureVecs(textlist, seq_len, model):\n",
    "    # Given a set of documents (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    #find the num of features\n",
    "    num_features = model.wv.syn0.shape[1]\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 4D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(textlist), seq_len, num_features, 1),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for example in textlist:\n",
    "        # \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(example, model, seq_len, num_features)\n",
    "        #\n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, model):\n",
    "    textlist = []\n",
    "    padded_textlist = []\n",
    "    print(\"Creating textlist\")\n",
    "    for text in df[\"text\"]:\n",
    "        textlist.append(text_to_wordlist(text, remove_stopwords=True ))\n",
    "        \n",
    "    seq_len = max(len(x)for x in textlist)\n",
    "    print(\"max seq length = \",seq_len)\n",
    "    print(\"padding textlist \")\n",
    "    for i in range(len(textlist)):\n",
    "        text = textlist[i]\n",
    "        num_padding = seq_len - len(text)\n",
    "        new_text = text + [\"\"] * num_padding\n",
    "        padded_textlist.append(new_text)\n",
    "        \n",
    "    textlist = padded_textlist\n",
    "    print(\"creating feature vecs\")    \n",
    "    DataVecs = getFeatureVecs(textlist, seq_len, model)\n",
    "    print (\"done!!!\")\n",
    "    return DataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
