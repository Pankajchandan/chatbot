{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@surmenok/natural-language-pipeline-for-chatbots-897bda41482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk.data\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_data):\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_data) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split() \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #    a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]  \n",
    "    # \n",
    "    # 5. autocorrect spellings\n",
    "    #auto_correct = [spell(w) for w in meaningful_words]\n",
    "    # \n",
    "    # 6. use stemmer to stem\n",
    "    stem_words = [snowball_stemmer.stem(w) for w in meaningful_words]\n",
    "    #\n",
    "    # 7. use lemmatizer to lemmatize the words\n",
    "    lemma_words = [wordnet_lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    #\n",
    "    # 8. Join the words back into one string separated by space, \n",
    "    #    and return the result.\n",
    "    return( \" \".join(lemma_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_preprocess(df):\n",
    "    #Get the number of reviews based on the dataframe column size\n",
    "    num_text = df[\"text\"].size\n",
    "\n",
    "    # Initialize an empty list to hold the clean texts\n",
    "    clean_train_text = []\n",
    "\n",
    "    # Loop over each review; create an index i that goes from 0 to the length\n",
    "    # of the text list \n",
    "    for i in range( 0, num_text ):\n",
    "        # Call our function for each one, and add the result to the list of\n",
    "        # clean reviews\n",
    "        clean_train_text.append(clean_text(df[\"text\"][i]))\n",
    "        \n",
    "    return clean_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False):\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split() \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #    a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # \n",
    "    # 4. Remove stop words\n",
    "    if remove_stopwords:\n",
    "        meaningful_words = [w for w in words if not w in stops]  \n",
    "    else:\n",
    "        meaningful_words = words\n",
    "    # \n",
    "    # 5. autocorrect spellings\n",
    "    #auto_correct = [spell(w) for w in meaningful_words]\n",
    "    # \n",
    "    # 6. use stemmer to stem\n",
    "    stem_words = [snowball_stemmer.stem(w) for w in meaningful_words]\n",
    "    #\n",
    "    # 7. use lemmatizer to lemmatize the words\n",
    "    lemma_words = [wordnet_lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    #\n",
    "    # 8. Join the words back into one string separated by space, \n",
    "    #    and return the result.\n",
    "    return(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "# we needd sentences because word2vec takes sentences as input. It leverages the SBD\n",
    "def text_to_sentences(text, tokenizer,remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call text_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(raw_sentence,remove_stopwords))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_and_clean_sentences(df):\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "    print (\"Parsing sentences from training set\")\n",
    "    for text in df[\"text\"]:\n",
    "        sentences += text_to_sentences(text, tokenizer)\n",
    "    print (\"parsing done!\")   \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save word2vec\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
    "\n",
    "* Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "* Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "* Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "* Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "* Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "* Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "* Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word2vec(df, num_features = 300, min_word_count = 1, num_workers = 4, context = 4, downsampling = 1e-3):\n",
    "    \n",
    "    from gensim.models import word2vec\n",
    "    # parse and clean sentence\n",
    "    sentences = parse_and_clean_sentences(df)\n",
    "    \n",
    "    #initialize logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "\n",
    "    print (\"Training model...\")\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers,size=num_features, min_count = min_word_count,\n",
    "                              window = context, sample = downsampling)\n",
    "\n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # save the model for later use.can load it later using Word2Vec.load()\n",
    "    model_name = \"trainedWord2vecmodel\"\n",
    "    model.save(model_name)\n",
    "    print (\"model saved as\", model_name)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 4           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##import dataset\n",
    "df = pd.read_csv(\"test.csv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-03 20:52:43,944 : INFO : collecting all words and their counts\n",
      "2017-11-03 20:52:43,945 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-03 20:52:43,946 : INFO : collected 20 word types from a corpus of 26 raw words and 4 sentences\n",
      "2017-11-03 20:52:43,947 : INFO : Loading a fresh vocabulary\n",
      "2017-11-03 20:52:43,948 : INFO : min_count=1 retains 20 unique words (100% of original 20, drops 0)\n",
      "2017-11-03 20:52:43,949 : INFO : min_count=1 leaves 26 word corpus (100% of original 26, drops 0)\n",
      "2017-11-03 20:52:43,950 : INFO : deleting the raw counts dictionary of 20 items\n",
      "2017-11-03 20:52:43,951 : INFO : sample=0.001 downsamples 20 most-common words\n",
      "2017-11-03 20:52:43,952 : INFO : downsampling leaves estimated 4 word corpus (15.9% of prior 26)\n",
      "2017-11-03 20:52:43,953 : INFO : estimated required memory for 20 words and 300 dimensions: 58000 bytes\n",
      "2017-11-03 20:52:43,954 : INFO : resetting layer weights\n",
      "2017-11-03 20:52:43,955 : INFO : training model with 4 workers on 20 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-11-03 20:52:43,958 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-03 20:52:43,959 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-03 20:52:43,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-03 20:52:43,961 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-03 20:52:43,963 : INFO : training on 130 raw words (25 effective words) took 0.0s, 5556 effective words/s\n",
      "2017-11-03 20:52:43,963 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-03 20:52:43,964 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-11-03 20:52:43,966 : INFO : saving Word2Vec object under trainedWord2vecmodel, separately None\n",
      "2017-11-03 20:52:43,967 : INFO : not storing attribute syn0norm\n",
      "2017-11-03 20:52:43,968 : INFO : not storing attribute cum_table\n",
      "2017-11-03 20:52:43,970 : INFO : saved trainedWord2vecmodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "parsing done!\n",
      "Training model...\n",
      "model saved as trainedWord2vecmodel\n"
     ]
    }
   ],
   "source": [
    "model = train_word2vec(df, num_features, min_word_count, num_workers, context, downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wher', 0.12596893310546875),\n",
       " ('are', 0.08614814281463623),\n",
       " ('shim', 0.06376539170742035),\n",
       " ('tell', 0.06363306939601898),\n",
       " ('have', 0.05454188585281372),\n",
       " ('me', 0.04825294017791748),\n",
       " ('is', 0.03167020156979561),\n",
       " ('a', 0.017723508179187775),\n",
       " ('when', 0.01742667891085148),\n",
       " ('can', 0.006341244094073772)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-03 21:18:30,746 : INFO : loading Word2Vec object from trainedWord2vecmodel\n",
      "2017-11-03 21:18:30,751 : INFO : loading wv recursively from trainedWord2vecmodel.wv.* with mmap=None\n",
      "2017-11-03 21:18:30,752 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-03 21:18:30,754 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-03 21:18:30,756 : INFO : loaded trainedWord2vecmodel\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"trainedWord2vecmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.wv.syn0.shape\n",
    "# model.wv.index2word\n",
    "# model[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##A simple way to assign a word2vec vector to a document is to take a mean of its words.\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
